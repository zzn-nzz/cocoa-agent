# Contributing to CocoaBench

Thank you for your interest in contributing to CocoaBench! ğŸ‰

We'd love to have your help in building a diverse and challenging benchmark. The best tasks come from real problems you've encountered â€” if it challenged you, it'll likely challenge AI agents too! Contributors with **3 accepted tasks** are eligible for co-authorship on the CocoaBench paper, which we plan to submit to a top-tier ML conference. We'll work with you through iterative refinement to help get your tasks accepted and ensure they meet benchmark standards. Particularly interesting or creative tasks may count for more at the discretion of project leads.

This guide will walk you through the process of creating and submitting a new task. Don't worry if it's your first time â€” we've tried to make it as straightforward as possible.

> ğŸ’¡ **Quick tip:** Please encrypt your task files before submitting. This keeps our benchmark data safe from being found by LLM agents that can search online, which helps ensure fair evaluation for everyone.


## Quick Start

```bash
cd contrib

# 1. Create your task using the wizard
python create_task.py

# 2. Validate your task
python validate_task.py your-task-name

# 3. Test difficulty with an AI agent
#    Recommended: Gemini 3 Pro, ChatGPT Agent, Claude 4.5
#    At least one agent should fail â€” that's what makes a great benchmark task!
#    Update evaluation.md with results and chat transcript link

# 4. Encrypt before submitting PR
python encrypt_tasks.py --task your-task-name

# 5. Validate encryption
python validate_task.py your-task-name --check-encrypted

# 6. Submit Pull Request
```

---

## What Makes a Good Task?

Great tasks tend to share these qualities:

- ğŸ§© Require **multi-step solutions** â€” not just a single lookup
- âœ“ Have **clear, verifiable answers** â€” so we can evaluate automatically
- ğŸŒ Involve **web browsing, visual perception, or file processing**
- ğŸ”§ Combine **multiple tools** (e.g., search + calculation + code)

Feel free to browse our [example tasks](https://cocoabench.github.io/#examples) for inspiration!

### Things to Keep in Mind

- âŒ Too easy (directly solvable by ChatGPT with searching)
- âŒ Time-sensitive data that will become stale (e.g., the solution relies on a website that will likely update)
- âŒ Subjective or opinion-based answers
- âŒ Impossible to evaluate automatically
- âŒ Require excessive resources or paid APIs

---

## Step-by-Step Guide

Here's a detailed walkthrough of creating your task. Take your time â€” quality matters more than speed!

### Step 1: Create Your Task Files

All commands should be run from the `contrib/` directory:

```bash
cd contrib
```

**Option A: Use the Task Wizard (Recommended)**
```bash
python create_task.py
```
The wizard will guide you through creating all required files and save them to `cocoabench-head/your-task-name/`.

**Option B: Create files manually**

```bash
mkdir -p ../cocoabench-head/your-task-name
```

Then create these files in your task folder:

- `instruction.md` - Task prompt for the AI agent
- `evaluation.md` - Expected answer and evaluation criteria
- `solution.md` - Step-by-step human solution
- `metadata.json` - Task metadata

See the [File Specifications](#file-specifications) below or check `cocoabench-example-tasks/` for examples.

### Step 2: Validate Your Task

```bash
python validate_task.py your-task-name
```

### Step 3: Test Difficulty with an AI Agent (Required)

> ğŸ¯ **Goal:** At least one AI agent should fail to solve your task correctly. This is what makes a benchmark task valuable â€” if all agents can easily solve it, it won't help us measure progress!

**Recommended agents:** Gemini 3 Pro, ChatGPT Agent, Claude 4.5

After testing, update `evaluation.md` with the agent's performance and **include a link to the chat transcript**. If all agents succeed, consider making the task more challenging â€” we're happy to help refining great task ideas!

### Step 4: Encrypt Your Task

Almost there! Just one more step before submitting â€” encrypting your task. This keeps our benchmark data safe and ensures fair evaluation for everyone.

```bash
python encrypt_tasks.py --task your-task-name
```

This will:
- Encrypt `instruction.md` â†’ `instruction.md.enc`
- Encrypt `evaluation.md` â†’ `evaluation.md.enc`
- Encrypt `solution.md` â†’ `solution.md.enc`
- Encrypt `metadata.json` â†’ `metadata.json.enc`
- Create `canary.txt`
- Remove the original files

### Step 5: Validate Encryption

```bash
python validate_task.py your-task-name --check-encrypted
```

### Step 6: Submit Pull Request

You're ready to share your work! ğŸ‰

```bash
git checkout -b task/your-task-name
git add cocoabench-head/your-task-name/
git commit -m "Add task: your-task-name"
git push origin task/your-task-name
```

Then open a PR on GitHub with:
- **Title:** `[New Task] your-task-name`
- **Description:** What your task is about, what skills it tests, and any interesting challenges you encountered

---

## Best Practices

1. **Ensure tasks are deterministic** â€“ Same input should always yield same answer
2. **Include clear success criteria** â€“ The answer should be unambiguous
3. **Test your task thoroughly** â€“ Solve it yourself before submission

---

## Need Help?

We're here to help! Here are some resources:

- ğŸ“‚ Check existing tasks in `cocoabench-example-tasks/` for reference
- ğŸ” Run `python validate_task.py --all` to explore how other tasks are structured

If you have questions or run into issues, feel free to open an issue on GitHub. We're happy to help!

Thanks for contributing â€” we really appreciate it! ğŸ™

---

# Reference

## Task Structure

Your contributed tasks will go in the `cocoabench-head/` folder. (The `cocoabench-example-tasks/` folder contains reference examples you can learn from.)

**Before encryption:**
```
cocoabench-head/
â””â”€â”€ your-task-name/
    â”œâ”€â”€ instruction.md        # Task instruction (required)
    â”œâ”€â”€ evaluation.md         # Evaluation criteria (required)
    â”œâ”€â”€ solution.md           # Solution walkthrough (required)
    â”œâ”€â”€ metadata.json         # Task metadata (required)
    â”œâ”€â”€ Dockerfile            # Container setup (optional)
    â”œâ”€â”€ docker-compose.yaml   # Docker config (optional)
    â””â”€â”€ assets/               # Resource URLs or files (optional)
        â””â”€â”€ urls.txt          # URLs to download files
```

**After encryption (ready for PR):**
```
cocoabench-head/
â””â”€â”€ your-task-name/
    â”œâ”€â”€ instruction.md.enc    # Encrypted instruction
    â”œâ”€â”€ evaluation.md.enc     # Encrypted evaluation
    â”œâ”€â”€ solution.md.enc       # Encrypted solution
    â”œâ”€â”€ metadata.json.enc     # Encrypted metadata
    â”œâ”€â”€ canary.txt            # Encryption key
    â”œâ”€â”€ Dockerfile            # (unchanged)
    â”œâ”€â”€ docker-compose.yaml   # (unchanged)
    â””â”€â”€ assets/               # (unchanged, e.g., urls.txt)
```

---

## File Specifications

### 1. `instruction.md` â€“ The Task Prompt

This file contains **only** the task prompt. It should be ready to send directly to an AI agent.

**Rules:**
- âœ… Start directly with the task description
- âœ… Use clear, unambiguous language
- âœ… Include all necessary context and constraints
- âœ… Always end with an **Output Format** section using `<answer>` tags

**Template:**

```markdown
[Task description - clearly explain what the agent should do, 
including all necessary context and constraints]

**Output Format:**

Submit your answer in the following format:

\`\`\`
<answer>your_answer_here</answer>
\`\`\`
```

---

### 2. `evaluation.md` â€“ Evaluation Criteria

Contains the expected answer and initialization resources.

**Template:**

```markdown
# Evaluation for Task [ID]

## Initialization

[Required resources: "Local: assets/", "Host UI: url", or "None"]

## Evaluation Criteria

[The correct answer - exact value, tolerance range, or matching criteria]

## Agent Output Example

[Agent name]: [result], ([Correct/Incorrect], [time])
Chat transcript: [link to chat]
```

> ğŸ¯ **Goal:** At least one agent should fail! Test with one agent (recommended: Gemini 3 Pro, ChatGPT Agent, or Claude 4.5) that fails and include the chat transcript link.

**âš ï¸ Evaluation Criteria Rule:**

The final output of an agent should be easy to evaluate programmatically, ideally as a **simple string or number** that can be verified using a lightweight Python script. This enables reliable automatic evaluation without requiring complex verification logic.

- âœ… Good: `$52.10` or `London` or `{"a": 1, "b": 3}`
- âœ… Good: Numeric with tolerance or string with fuzzy match: `2.8 (Â±0.1)`
- âŒ Avoid: Open-ended output that require human evaluation
- âŒ Avoid: Tasks that need a very complex evaluation scripts to verify correctness

**Initialization Types:**

| Type | Format |
|------|--------|
| Google Drive file | `Image: https://drive.google.com/file/d/...` |
| Google Drive folder | `Folder: https://drive.google.com/drive/folders/...` |
| Web UI | `Host UI: https://example.com/page` |
| Local files | `Local: assets/` |
| None needed | `None` |

**Multiple Resources:**

When a task requires multiple resources, list each on a separate line with clear labels:

```markdown
## Initialization

- Image: https://drive.google.com/file/d/abc123/view
- Data: https://drive.google.com/file/d/xyz789/view
- Host UI: https://example.com/app
```

---

### 3. `solution.md` â€“ Human Solution Walkthrough

Documents how a human would solve the task step-by-step.

**Template:**

```markdown
# Solution

### Step 1: [Step Title]
[Detailed explanation with sub-steps if needed]

### Final Answer
[The correct answer]
```

---

### 4. `metadata.json` â€“ Task Metadata

**Template:**

```json
{
  "id": 99,
  "name": "your-task-name",
  "brainstorm_by": "Your Name",
  "stage": "Brainstorm",
  "self_checked": "no",
  "reviewers": {},
  "task_properties": {},
  "human_performance": {}
}
```

**Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Unique task ID (wizard auto-generates) |
| `name` | string | Matches folder name (kebab-case) |
| `brainstorm_by` | string | Your name |
| `stage` | string | `"Brainstorm"` â†’ `"Approved"` (after review) |
| `self_checked` | string | `"yes"` after you've verified the solution |
| `reviewers` | object | Filled by reviewers: `{"reviewer_1_name": "Pass"}` |
