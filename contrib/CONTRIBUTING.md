# Contributing to CocoaBench

Thank you for your interest in contributing to CocoaBench! ğŸ‰

We'd love to have your help in building a diverse and challenging benchmark. The best tasks come from real problems you've encountered â€” if it challenged you, it'll likely challenge AI agents too! Contributors with **3 accepted tasks** are eligible for co-authorship on the CocoaBench paper, which we plan to submit to a top-tier ML conference. We'll work with you through iterative refinement to help get your tasks accepted and ensure they meet benchmark standards. Particularly interesting or creative tasks may count for more at the discretion of project leads.

This guide will walk you through the process of creating and submitting a new task. Don't worry if it's your first time â€” we've tried to make it as straightforward as possible.

> ğŸ’¡ **Quick tip:** Please encrypt your task files before submitting. This keeps our benchmark data safe from being found by LLM agents that can search online, which helps ensure fair evaluation for everyone.


## Quick Start

```bash
cd contrib

# 1. Create your task using the wizard
python create_task.py

# 2. Validate your task
python validate_task.py your-task-name

# 3. Encrypt before submitting PR
python encrypt_tasks.py --task your-task-name

# 4. Validate encryption
python validate_task.py your-task-name --check-encrypted

# 5. Submit Pull Request
```

---

## What Makes a Good Task?

Great tasks tend to share these qualities:

- ğŸ§© Require **multi-step solutions** â€” not just a single lookup
- âœ“ Have **clear, verifiable answers** â€” so we can evaluate automatically
- ğŸŒ Involve **web browsing, visual perception, or file processing**
- ğŸ”§ Combine **multiple tools** (e.g., search + calculation + code)

Feel free to browse our [example tasks](https://cocoabench.github.io/#examples) for inspiration!

### Things to Keep in Mind

- âŒ Too easy (directly solvable by ChatGPT with searching)
- âŒ Time-sensitive data that will become stale (e.g., the solution relies on a website that will likely update)
- âŒ Subjective or opinion-based answers
- âŒ Impossible to evaluate automatically
- âŒ Require excessive resources or paid APIs

---

## Task Structure

Your contributed tasks will go in the `contributed-tasks/` folder. (The `cocoabench-example-tasks/` folder contains reference examples you can learn from.)

**Before encryption:**
```
contributed-tasks/
â””â”€â”€ your-task-name/
    â”œâ”€â”€ instruction.md        # Task instruction (required)
    â”œâ”€â”€ evaluation.md         # Evaluation criteria (required)
    â”œâ”€â”€ solution.md           # Solution walkthrough (required)
    â”œâ”€â”€ metadata.json         # Task metadata (required)
    â”œâ”€â”€ Dockerfile            # Container setup (optional)
    â”œâ”€â”€ docker-compose.yaml   # Docker config (optional)
    â””â”€â”€ assets/               # Resource URLs or files (optional)
        â””â”€â”€ urls.txt          # URLs to download files
```

**After encryption (ready for PR):**
```
contributed-tasks/
â””â”€â”€ your-task-name/
    â”œâ”€â”€ instruction.md.enc    # Encrypted instruction
    â”œâ”€â”€ evaluation.md.enc     # Encrypted evaluation
    â”œâ”€â”€ solution.md.enc       # Encrypted solution
    â”œâ”€â”€ metadata.json.enc     # Encrypted metadata
    â”œâ”€â”€ canary.txt            # Encryption key
    â”œâ”€â”€ Dockerfile            # (unchanged)
    â”œâ”€â”€ docker-compose.yaml   # (unchanged)
    â””â”€â”€ assets/               # (unchanged, e.g., urls.txt)
```

---

## Step-by-Step Guide

Here's a detailed walkthrough of creating your task. Take your time â€” quality matters more than speed!

### Step 1: Create Your Task Files

All commands should be run from the `contrib/` directory:

```bash
cd contrib
```

**Option A: Use the Task Wizard (Recommended)**
```bash
python create_task.py
```
The wizard will guide you through creating all required files and save them to `contributed-tasks/your-task-name/`.

**Option B: Create files manually**

```bash
mkdir -p ../contributed-tasks/your-task-name
```

Then create these files in your task folder:

- `instruction.md` - Task prompt for the AI agent
- `evaluation.md` - Expected answer and evaluation criteria
- `solution.md` - Step-by-step human solution
- `metadata.json` - Task metadata

See the templates below or check `cocoabench-example-tasks/` for examples.

### Step 2: Validate Your Task

```bash
python validate_task.py your-task-name
```

### Step 3: Test with an AI Agent (Optional)

Run your task with an agent to verify it works as expected.

### Step 4: Encrypt Your Task

Almost there! Just one more step before submitting â€” encrypting your task. This keeps our benchmark data safe and ensures fair evaluation for everyone.

```bash
python encrypt_tasks.py --task your-task-name
```

This will:
- Encrypt `instruction.md` â†’ `instruction.md.enc`
- Encrypt `evaluation.md` â†’ `evaluation.md.enc`
- Encrypt `solution.md` â†’ `solution.md.enc`
- Encrypt `metadata.json` â†’ `metadata.json.enc`
- Create `canary.txt`
- Remove the original files

### Step 5: Validate Encryption

```bash
python validate_task.py your-task-name --check-encrypted
```

### Step 6: Submit Pull Request

You're ready to share your work! ğŸ‰

```bash
git checkout -b task/your-task-name
git add contributed-tasks/your-task-name/
git commit -m "Add task: your-task-name"
git push origin task/your-task-name
```

Then open a PR on GitHub. We'd love to hear:
- What your task is about
- What skills it tests
- Any interesting challenges you encountered while creating it

---

## Submitting a Pull Request

Once your task is ready and encrypted:

1. **Fork the repository** (if you haven't already)

2. **Create a new branch** for your task:
   ```bash
   git checkout -b task/<your-task-name>
```

3. **Commit your changes**:
   ```bash
   git add contributed-tasks/<your-task-name>/
   git commit -m "Add task: <your-task-name>"
   ```

4. **Push to your fork**:
   ```bash
   git push origin task/<your-task-name>
   ```

5. **Open a Pull Request** on GitHub with:
   - A clear title: `[New Task] <your-task-name>`
   - Description of what the task tests
   - Confirmation that you've validated and encrypted the task

6. **Wait for review** â€” we'll take a look and may have some feedback. Thanks for your patience!

---

## Useful Commands

```bash
# Run from the contrib/ directory
cd contrib

# Create a new task interactively
python create_task.py

# Validate a specific task
python validate_task.py <task-name>

# Validate all tasks
python validate_task.py --all

# Encrypt a specific task
python encrypt_tasks.py --task <task-name>

# Encrypt all tasks
python encrypt_tasks.py
```

---

## File Specifications

### 1. `instruction.md` â€“ The Task Prompt

This file contains **only** the task prompt. It should be ready to send directly to an AI agent.

**Rules:**
- âœ… Start directly with the task description
- âœ… Use clear, unambiguous language
- âœ… Include all necessary context and constraints
- âœ… Always end with an **Output Format** section using `<answer>` tags

**Template:**

```markdown
[Task description - clearly explain what the agent should do, 
including all necessary context and constraints]

**Output Format:**

Submit your answer in the following format:

\`\`\`
<answer>your_answer_here</answer>
\`\`\`
```

---

### 2. `evaluation.md` â€“ Evaluation Criteria

Contains the expected answer and initialization resources.

**Template:**

```markdown
# Evaluation for Task [ID]

## Initialization

[Required resources: "Local: assets/", "Host UI: url", or "None"]

## Evaluation Criteria

[The correct answer - exact value, tolerance range, or matching criteria]

## Agent Output Example

[To be filled after agent testing]
```

**âš ï¸ Evaluation Criteria Rule:**

The evaluation criteria should ideally be a **simple string or number** that can be directly matched against agent output. This enables automatic evaluation without complex scripts.

- âœ… Good: `Jason Wei, 9` or `$56.36` or `London`
- âœ… Good: Numeric with tolerance: `2.8 (Â±0.1)`
- âŒ Avoid: Complex prose descriptions that require human judgment
- âŒ Avoid: Tasks that need complex evaluation scripts to verify correctness

**Initialization Types:**

| Type | Format |
|------|--------|
| Image file | `Image: https://drive.google.com/file/d/...` |
| Web UI | `Host UI: https://example.com/page` |
| Google Drive folder | `Folder: https://drive.google.com/drive/folders/...` |
| Local files | `Local: assets/` |
| None needed | `None` or `None (web browsing only)` |

**Multiple Resources:**

When a task requires multiple resources, list each on a separate line with clear labels:

```markdown
## Initialization

- Image: https://drive.google.com/file/d/abc123/view
- Data: https://drive.google.com/file/d/xyz789/view
- Host UI: https://example.com/app
```

Or use a structured format:

```markdown
## Initialization

| Resource | URL/Path |
|----------|----------|
| Input Image | https://drive.google.com/file/d/abc123/view |
| Reference Data | https://drive.google.com/file/d/xyz789/view |
| Web Portal | https://example.com/app |
```

---

### 3. `solution.md` â€“ Human Solution Walkthrough

Documents how a human would solve the task step-by-step.

**Template:**

```markdown
# Solution

### Step 1: [Step Title]
[Detailed explanation with sub-steps if needed]

### Final Answer
[The correct answer]
```

---

### 4. `metadata.json` â€“ Task Metadata

**Template:**

```json
{
  "id": 99,
  "name": "your-task-name",
  "brainstorm_by": "Your Name",
  "stage": "Brainstorm",
  "self_checked": "no",
  "reviewers": {},
  "task_properties": {},
  "human_performance": {}
}
```

**Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Unique task ID (wizard auto-generates) |
| `name` | string | Matches folder name (kebab-case) |
| `brainstorm_by` | string | Your name |
| `stage` | string | `"Brainstorm"` â†’ `"Approved"` (after review) |
| `self_checked` | string | `"yes"` after you've verified the solution |
| `reviewers` | object | Filled by reviewers: `{"reviewer_1_name": "Pass"}` |

---

## Output Format Standards

All tasks must use one of these standardized output formats:

### Simple Answer Tag (Most Common)
```
<answer>value</answer>
```

### Multi-line Answer Tag
```
<answer>
Line 1
Line 2
</answer>
```

### JSON Output
```json
{
  "field": "value"
}
```

### File Output
Specify exact file path and format in the instruction.

---

## Best Practices

1. **Ensure tasks are deterministic** â€“ Same input should always yield same answer
2. **Include clear success criteria** â€“ The answer should be unambiguous
3. **Test your task thoroughly** â€“ Solve it yourself before submission

---

## Review Process

1. **Brainstorm** â€“ Create your task with `"stage": "Brainstorm"`
2. **Self-Check** â€“ Verify solution works, set `"self_checked": "yes"`
3. **Peer Review** â€“ Another contributor validates and adds to `reviewers`
4. **Approval** â€“ After passing review, stage changes to `"Approved"`

---

## Task Categories

Consider which category your task fits:

| Category | Examples |
|----------|----------|
| **Research & Analysis** | Paper analysis, citation networks, genealogy |
| **Visual Perception** | Image recognition, country identification, figure analysis |
| **Web Navigation** | Multi-step web tasks, form filling, data extraction |
| **Data Processing** | CSV analysis, statistical computation, visualization |
| **Reasoning** | Constraint satisfaction, optimization, logical deduction |
| **Domain Knowledge** | Sports, music, pharmaceuticals, AI/ML trends |

---

## Need Help?

We're here to help! Here are some resources:

- ğŸ“‚ Check existing tasks in `cocoabench-example-tasks/` for reference
- ğŸ“– See `TASKS_WITH_INITIALIZATION.md` for initialization patterns
- ğŸ” Run `python validate_task.py --all` to explore how other tasks are structured

If you have questions or run into issues, feel free to open an issue on GitHub. We're happy to help!

Thanks for contributing â€” we really appreciate it! ğŸ™

